
🎯Project Description

The Speech Emotion Recognition (SER) project aims to develop a system that detects emotions from speech signals in real-time using machine learning models. It involves data collection, preprocessing, feature extraction, model training, evaluation, and deployment for real-time applications.
Features

    🔊Mel Frequency Cepstral Coefficients (MFCCs): Captures spectral characteristics of the audio signal.
    🎼Formant Frequencies: Represents resonance frequencies that correlate with vocal tract shape and emotional states.
    🎚️Pitch and Pitch Contour: Analyzes variations in the fundamental frequency of the speech signal.
    ⏱️Speech Rate: Measures the rate of speech to detect emotional arousal.
    🔋Energy: Reflects the intensity or loudness of the speech signal.
    ⚡Zero Crossing Rate: Captures the temporal dynamics of the speech signal.

Project Workflow

    📂Data Collection: Gather labeled speech samples for training.
    🧹Preprocessing: Standardize and clean the audio data for consistency.
    Feature Extraction: Extract key features like MFCCs, pitch, formant frequencies, etc.
    🧠Model Training: Train machine learning models such as SVM, CNN, or RNN.
    Model Evaluation: Assess performance using accuracy and F1-score.

Dependencies

    Python 3.x
    Libraries: numpy, scikit-learn, librosa, tensorflow or pytorch, google.colab
